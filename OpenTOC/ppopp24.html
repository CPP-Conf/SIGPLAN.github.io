<html xmlns:bkstg="http://www.atypon.com/backstage-ns" xmlns:urlutil="java:com.atypon.literatum.customization.UrlUtil" xmlns:pxje="java:com.atypon.frontend.services.impl.PassportXslJavaExtentions"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="Content-Style-Type" content="text/css"><style type="text/css">
            #DLtoc {
            font: normal 12px/1.5em Arial, Helvetica, sans-serif;
            }

            #DLheader {
            }
            #DLheader h1 {
            font-size:16px;
            }

            #DLcontent {
            font-size:12px;
            }
            #DLcontent h2 {
            font-size:14px;
            margin-bottom:5px;
            }
            #DLcontent h3 {
            font-size:12px;
            padding-left:20px;
            margin-bottom:0px;
            }

            #DLcontent ul{
            margin-top:0px;
            margin-bottom:0px;
            }

            .DLauthors li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLauthors li:after{
            content:",";
            }
            .DLauthors li.nameList.Last:after{
            content:"";
            }

            .DLabstract {
            padding-left:40px;
            padding-right:20px;
            display:block;
            }

            .DLformats li{
            display: inline;
            list-style-type: none;
            padding-right: 5px;
            }

            .DLformats li:after{
            content:",";
            }
            .DLformats li.formatList.Last:after{
            content:"";
            }

            .DLlogo {
            vertical-align:middle;
            padding-right:5px;
            border:none;
            }

            .DLcitLink {
            margin-left:20px;
            }

            .DLtitleLink {
            margin-left:20px;
            }

            .DLotherLink {
            margin-left:0px;
            }

        </style><title>PPoPP '24: Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming</title></head><body><div id="DLtoc"><div id="DLheader"><h1>PPoPP '24: Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming</h1><a class="DLcitLink" title="Go to the ACM Digital Library for additional information about this proceeding" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/proceedings/10.1145/3627535"><img class="DLlogo" alt="Digital Library logo" height="30" src="https://dl.acm.org/specs/products/acm/releasedAssets/images/footer-logo1.png">
                Full Citation in the ACM Digital Library
            </a></div><div id="DLcontent"><h2>SESSION: Keynote</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638568">Sparsity in Deep Neural Nets (Keynote)</a></h3><ul class="DLauthors"><li class="nameList Last">Nir N Shavit</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Our brain executes very sparse computation, allowing for great speed and energy savings. Deep neural networks can also be made to exhibit high levels of sparsity without significant accuracy loss. As their size grows, it is becoming imperative that we use sparsity to improve their efficiency. This is a challenging task because the memory systems and SIMD operations that dominate todays CPUs and GPUs do not lend themselves easily to the irregular data patterns sparsity introduces. This talk will survey the role of sparsity in neural network computation, and the parallel algorithms and hardware features that nevertheless allow us to make effective use of it.</p>
			</div></div>
						
					<h2>SESSION: Synchronization and Concurrency Control I</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638472">Scaling Up Transactions with Slower Clocks</a></h3><ul class="DLauthors"><li class="nameList">Pedro Ramalhete</li><li class="nameList Last">Andreia Correia</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Concurrency controls with optimistic read accesses and pessimistic write accesses are among the fastest in the literature. However, during write transactions these algorithms need to increment an atomic variable, the central clock, limiting parallelism and preventing scalability at high core counts.</p>
				<p>In this paper, we propose a new concurrency control, Deferred Clock Transactional Locking (DCTL), which significantly reduces the heartbeat of the central clock, thus increasing scalability. DCTL will not increment the clock for consecutive disjoint transactions. An optimized variant, named DCOTL, allows for consecutive transactions with nondisjoint write-accesses to commit without incrementing the clock. Moreover, we show variants of these two algorithms with starvation-free transactions.</p>
				<p>Transactions in DCTL are opaque, which means it can be applied to concurrent data structures, Database Management Systems, Software Transactional Memory, and Persistent Transactional Memory. Our experiments show that these DCTL algorithms match or surpass the current state of the art for most workloads. We adapted both algorithms using an existing durability technique and implemented a fully transactional DBMS with disk persistence, whose scalability in write transactions exceeds the current state of the art.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638477">Locks as a Resource: Fairly Scheduling Lock Occupation with CFL</a></h3><ul class="DLauthors"><li class="nameList">Jonggyu Park</li><li class="nameList Last">Young Ik Eom</li></ul><div class="DLabstract"><div style="display:inline">
				<p>In multi-container environments, applications oftentimes experience unexpected performance fluctuations due to undesirable interference among applications. Synchronization such as locks has been targeted as one of the reasons but still remains an uncontrolled resource while a large set of locks are still shared across applications. In this paper, we demonstrate that this lack of lock scheduling incurs significant real-world problems including performance unfairness and interference among applications. To address this problem, we propose a new synchronization design with an embedded scheduling capability, called CFL (Completely Fair Locking). CFL fairly distributes a fair amount of lock occupation time to applications considering their priorities and cgroup information. For scalability, CFL also considers the NUMA topology in the case of NUMA machines. Experimental results demonstrate that CFL significantly improves performance fairness while achieving comparable or sometimes even superior performance to state-of-the-art locks.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638491">Are Your Epochs Too Epic? Batch Free Can Be Harmful</a></h3><ul class="DLauthors"><li class="nameList">Daewoo Kim</li><li class="nameList">Trevor Brown</li><li class="nameList Last">Ajay Singh</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Epoch based memory reclamation (EBR) is one of the most popular techniques for reclaiming memory in lock-free and optimistic locking data structures, due to its ease of use and good performance in practice. However, EBR is known to be sensitive to thread delays, which can result in performance degradation. Moreover, the exact mechanism for this performance degradation is not well understood.</p>
				<p>This paper illustrates this performance degradation in a popular data structure benchmark, and does a deep dive to uncover its root cause---a subtle interaction between EBR and state of the art memory allocators. In essence, modern allocators attempt to reduce the overhead of freeing by maintaining bounded thread caches of objects for local reuse, actually freeing them (a very high latency operation) only when thread caches become too large. EBR immediately bypasses these mechanisms whenever a particularly large batch of objects is freed, substantially increasing overheads and latencies. Beyond EBR, many memory reclamation algorithms, and data structures, that reclaim objects in large batches suffer similar deleterious interactions with popular allocators.</p>
				<p>We propose a simple algorithmic fix for such algorithms to amortize the freeing of large object batches over time, and apply this technique to ten existing memory reclamation algorithms, observing performance improvements for nine out of ten, and over 50% improvement for six out of ten in experiments on a high performance lock-free ABtree. We also present an extremely simple token passing variant of EBR and show that, with our fix, it performs 1.5--2.6× faster than the fastest known memory reclamation algorithm, and 1.2--1.5× faster than not reclaiming at all, on a 192 thread four socket Intel system.</p>
			</div></div>
						
					<h2>SESSION: Compilers and Runtimes for Parallel Systems</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638466">Liger: Interleaving Intra- and Inter-Operator Parallelism for Distributed Large Model Inference</a></h3><ul class="DLauthors"><li class="nameList">Jiangsu Du</li><li class="nameList">Jinhui Wei</li><li class="nameList">Jiazhi Jiang</li><li class="nameList">Shenggan Cheng</li><li class="nameList">Dan Huang</li><li class="nameList">Zhiguang Chen</li><li class="nameList Last">Yutong Lu</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Distributed large model inference is still in a dilemma where balancing cost and effect. The online scenarios demand intraoperator parallelism to achieve low latency and intensive communications makes it costly. Conversely, the inter-operator parallelism can achieve high throughput with much fewer communications, but it fails to enhance the effectiveness.</p>
				<p>In this paper, we present Liger, a distributed large model inference runtime system that is of capability to achieve low latency at high throughput on the multi-GPU architecture. The key idea lies in the novel interleaved parallelism, which interleaves the computation and communication across requests. Liger enables this parallelism by carefully scheduling computation and communication kernels across requests onto multiple streams of multiple GPUs. It achieves precise control of kernel execution order efficiently by mixing use the CPU-GPU synchronization and the inter-stream synchronization. To prevent scheduling failures caused by resource contention, Liger introduces a contention factor strategy to anticipate the penalty of contention. It enables a higher degree of overlap by decomposing lengthy kernels into smaller, more manageable units at runtime.</p>
				<p>Extensive evaluations show that Liger, in most cases, outperforms existing parallelism approaches across models and devices, presenting the best latency and throughput results. In a 4-device case, Liger reduces the average latency by 36.0% while maintaining the same throughput compared to the inter-operator approach. Meanwhile, it improves the throughput by 1.34× with improved average latency compared to the intra-operator approach.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638484">A Holistic Approach to Automatic Mixed-Precision Code Generation and Tuning for Affine Programs</a></h3><ul class="DLauthors"><li class="nameList">Jinchen Xu</li><li class="nameList">Guanghui Song</li><li class="nameList">Bei Zhou</li><li class="nameList">Fei Li</li><li class="nameList">Jiangwei Hao</li><li class="nameList Last">Jie Zhao</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Reducing floating-point (FP) precision is used to trade the quality degradation of a numerical program's output for performance, but this optimization coincides with type casting, whose overhead is undisclosed until a mixed-precision code version is generated. This uncertainty enforces the decoupled implementation of mixed-precision code generation and autotuning in prior work. In this paper, we present a holistic approach called PrecTuner that consolidates the mixed-precision code generator and the autotuner by defining one parameter. This parameter is first initialized by some automatically sampled values and used to generate several code variants, with various loop transformations also taken into account. The generated code variants are next profiled to solve a performance model formulated using the aforementioned parameter, possibly under a pre-defined quality degradation budget. The best-performing value of the defined parameter is finally predicted without evaluating all code variants. Experimental results of the PolyBench benchmarks on CPU demonstrate that PrecTuner outperforms LuIs by 3.28× while achieving smaller errors, and we also validate its effectiveness in optimizing a real-life large-scale application. In addition, PrecTuner also obtains a mean speedup of 1.81× and 1.52×-1.73× over Pluto on single- and multi-core CPU, respectively, and 1.71× over PPCG on GPU.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638487">Language-Agnostic Static Deadlock Detection for Futures</a></h3><ul class="DLauthors"><li class="nameList Last">Stefan K Muller</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Deadlocks, in which threads wait on each other in a cyclic fashion and can't make progress, have plagued parallel programs for decades. In recent years, as the parallel programming mechanism known as <em>futures</em> has gained popularity, interest in preventing deadlocks in programs with futures has increased as well. Various static and dynamic algorithms exist to detect and prevent deadlock in programs with futures, generally by constructing some approximation of the dependency graph of the program but, as far as we are aware, all are specialized to a particular programming language.</p>
				<p>A recent paper introduced <em>graph types</em>, by which one can statically approximate the dependency graphs of a program in a language-independent fashion. By analyzing the graph type directly instead of the source code, a graph-based program analysis, such as one to detect deadlock, can be made language-independent. Indeed, the paper that proposed graph types also proposed a deadlock detection algorithm. Unfortunately, the algorithm was based on an unproven conjecture which we show to be false. In this paper, we present, and prove sound, a type system for finding possible deadlocks in programs that operates over graph types and can therefore be applied to many different languages. As a proof of concept, we have implemented the algorithm over a subset of the OCaml language extended with built-in futures.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638493">Recurrence Analysis for Automatic Parallelization of Subscripted Subscripts</a></h3><ul class="DLauthors"><li class="nameList">Akshay Bhosale</li><li class="nameList Last">Rudolf Eigenmann</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Introducing correct and optimal OpenMP parallelization directives in applications is a challenge. To parallelize a loop in an input application code automatically, parallelizing compilers need to disprove dependences with respect to variables across iterations of the loop. Performing such dependence analysis in the presence of index arrays or subscripted subscripts - <em>a</em>[<em>b</em>[<em>i</em>]] - has long been a challenge for automatic parallelizers. Loops with subscripted subscripts can be parallelized if the subscript array is known to possess a property such as monotonicity. This paper presents a compiletime algorithm that can analyze complex recurrence relations and determine irregular or <em>intermittent</em> monotonicity of one-dimensional and monotonicity of multi-dimensional subscript arrays. The new algorithm builds on a prior approach that is capable of analyzing simple recurrence relations and determining monotonic one-dimensional subscript arrays. Experimental results show that automatic parallelizers equipped with our new analysis techniques can substantially improve the performance of ten out of twelve or 83.33% of the benchmarks evaluated, 25--33.33% more than possible with state-of-the-art compile-time automatic parallelization techniques.</p>
			</div></div>
						
					<h2>SESSION: High Performance Computing</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638468">OsirisBFT: Say No to Task Replication for Scalable Byzantine Fault Tolerant Analytics</a></h3><ul class="DLauthors"><li class="nameList">Kasra Jamshidi</li><li class="nameList Last">Keval Vora</li></ul><div class="DLabstract"><div style="display:inline">
				<p>We present a verification-based Byzantine Fault Tolerant processing system, called OsirisBFT, for distributed task-parallel applications. OsirisBFT treats computation tasks differently from state update tasks, allowing the application to scale independently from number of expected failures. OsirisBFT captures application-specific verification semantics via generic <em>verification operators</em> and employs lightweight verification strategies with little coordination during graceful execution. Evaluation across multiple applications and workloads shows that OsirisBFT delivers high processing throughput and scalability compared to replicated processing. Importantly, the scalable nature of OsirisBFT enables it to reduce the performance gap compared to baseline with no fault tolerance by simply scaling out.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638473">Towards Scalable Unstructured Mesh Computations on Shared Memory Many-Cores</a></h3><ul class="DLauthors"><li class="nameList">Haozhong Qiu</li><li class="nameList">Chuanfu Xu</li><li class="nameList">Jianbin Fang</li><li class="nameList">Liang Deng</li><li class="nameList">Jian Zhang</li><li class="nameList">Qingsong Wang</li><li class="nameList">Yue Ding</li><li class="nameList">Zhe Dai</li><li class="nameList">Yonggang Che</li><li class="nameList">Shizhao Chen</li><li class="nameList Last">Jie Liu</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Due to <em>data conflicts</em> or <em>data dependences</em>, exploiting shared memory parallelism on unstructured mesh applications is highly challenging. The prior approaches are neither general nor scalable on emerging many-core processors. This paper presents a general and scalable shared memory approach for unstructured mesh computations. We recursively divide and reorder an unstructured mesh to construct a <em>task dependency tree</em> (TDT), where massive parallelism is exposed and data conflicts as well as data dependences are respected. We propose two recursion strategies to support popular programming models on both CPUs and GPUs for TDT. We evaluate our approach by applying it to an industrial unstructured Computational Fluid Dynamics (CFD) software. Experimental results show that our approach significantly outperforms the prior shared memory approaches, delivering up to 8.1× performance improvement over the engineer-tuned implementations.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638479">Extreme-scale Direct Numerical Simulation of Incompressible Turbulence on the Heterogeneous Many-core System</a></h3><ul class="DLauthors"><li class="nameList">Jiabin Xie</li><li class="nameList">Guangnan Feng</li><li class="nameList">Han Huang</li><li class="nameList">Junxuan Feng</li><li class="nameList">Zhiguang Chen</li><li class="nameList Last">Yutong Lu</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Direct numerical simulation (DNS) is a technique that directly solves the fluid Navier-Stokes equations with high spatial and temporal resolutions, which has driven much research regarding the nature of turbulence. For high-Reynolds number <em>(Re)</em> incompressible turbulence of particular interest, where the nondimensional <em>Re</em> characterizes the flow regime, the application of DNS is hindered by the fact that the numerical grid size (i.e., the memory requirement) scales with <em>Re</em><sup>3</sup>, while the overall computational cost scales with <em>Re</em><sup>4</sup>. Recent studies have shown that developing efficient parallel methods for heterogeneous many-core systems is promising to solve this computational challenge.</p>
				<p>We develop PowerLLEL++, a high-performance and scalable implicit finite difference solver for heterogeneous many-core systems, to accelerate the extreme-scale DNS of incompressible turbulence. To achieve this goal, an adaptive multi-level parallelization strategy is first proposed to fully exploit the multi-level parallelism and computing power of heterogeneous many-core systems. Second, hierarchical-memory-adapted data reuse/tiling strategy and kernel fusion are adopted to improve the performance of memory-bound stencil-like operations. Third, a parallel tridiagonal solver based on the parallel diagonal dominant (PDD) algorithm is developed to minimize the number of global data transposes. Fourth, three effective communication optimizations are implemented by Remote Direct Memory Access (RDMA) to maximize the performance of the remaining global transposes and halo exchange.</p>
				<p>Results show that the solver exploits the heterogeneous computing power of the new Tianhe supercomputer and achieves a speedup of up to 10.6× (against the CPU-only performance). Linear strong scaling is obtained with a grid size of up to 25.8 billion.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638503">Pure: Evolving Message Passing To Better Leverage Shared Memory Within Nodes</a></h3><ul class="DLauthors"><li class="nameList">James Psota</li><li class="nameList Last">Armando Solar-Lezama</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Pure is a new programming model and runtime system explicitly designed to take advantage of shared memory within nodes in the context of a <em>mostly</em> message passing interface enhanced with the ability to use tasks to make use of idle cores. Pure leverages shared memory in two ways: (a) by allowing cores to steal work from each other while waiting on messages to arrive, and, (b) by leveraging efficient lock-free data structures in shared memory to achieve high-performance messaging and collective operations between the ranks within nodes. We use microbenchmarks to evaluate Pure's key messaging and collective features and also show application speedups up to 2.1× on the CoMD molecular dynamics and the miniAMR adaptive mesh refinement applications scaling up to 4,096 cores.</p>
			</div></div>
						
					<h2>SESSION: Graph Processing</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638490">INFINEL: An efficient GPU-based processing method for unpredictable large output graph queries</a></h3><ul class="DLauthors"><li class="nameList">Sungwoo Park</li><li class="nameList">Seyeon Oh</li><li class="nameList Last">Min-Soo Kim</li></ul><div class="DLabstract"><div style="display:inline">
				<p>With the introduction of GPUs, which are specialized for iterative parallel computations, the execution of computation-intensive graph queries using a GPU has seen significant performance improvements. However, due to the memory constraints of GPUs, there has been limited research on handling large-scale output graph queries with unpredictable output sizes on a GPU. Traditionally, two-phase methods have been used, where the query is re-executed after splitting it into sub-tasks while only considering the size of the output in a static manner. However, two-phase methods become highly inefficient when used with graph data with extreme skew, failing to maximize the GPU performance. This paper proposes INFINEL, which handles unpredictable large output graph queries in a one-phase method through chunk allocation per thread and kernel stop/restart methods. We also propose applicable optimization techniques due to the corresponding unique characteristics of operating with low time/space overhead and not heavily relying on the GPU output buffer size. Through extensive experiments, we demonstrate that our one-phase method of INFINEL improves the performance by up to 31.5 times over the conventional two-phase methods for triangle listing ULO query.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638498">GraphCube: Interconnection Hierarchy-aware Graph Processing</a></h3><ul class="DLauthors"><li class="nameList">Xinbiao Gan</li><li class="nameList">Guang Wu</li><li class="nameList">Shenghao Qiu</li><li class="nameList">Feng Xiong</li><li class="nameList">Jiaqi Si</li><li class="nameList">Jianbin Fang</li><li class="nameList">Dezun Dong</li><li class="nameList">Chunye Gong</li><li class="nameList">Tiejun Li</li><li class="nameList Last">Zheng Wang</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Processing large-scale graphs with billions to trillions of edges requires efficiently utilizing parallel systems. However, current graph processing engines do not scale well beyond a few tens of computing nodes because they are oblivious to the communication cost variations across the interconnection hierarchy. We introduce GraphCube, a better approach to optimizing graph processing on large-scale parallel systems with complex interconnections. GraphCube features a new graph partitioning approach to achieve better load balancing and minimize communication overhead across multiple levels of the interconnection hierarchy. We evaluate GraphCube by applying it to fundamental graph operations performed on synthetic and real-world graph datasets. Our evaluation used up to 79,024 computing nodes and 1.2+ million processor cores. Our large-scale experiments show that GraphCube outperforms state-of-the-art parallel graph processing methods in throughput and scalability. Furthermore, GraphCube outperformed the top-ranked systems on the Graph 500 list.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638507">Exploiting Fine-Grained Redundancy in Set-Centric Graph Pattern Mining</a></h3><ul class="DLauthors"><li class="nameList">Zhiheng Lin</li><li class="nameList">Ke Meng</li><li class="nameList">Chaoyang Shui</li><li class="nameList">Kewei Zhang</li><li class="nameList">Junmin Xiao</li><li class="nameList Last">Guangming Tan</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Graph Pattern Mining (GPM) applications are memory intensive as they require a tremendous amount of edge checks. In recent years, the "set-centric" abstraction has gained attention for its powerful expressive abilities. By leveraging relational algebra, they optimized algorithms with methods like matching orders, early termination, automorphism-breaking, and result reuse to reduce redundancy. However, these approaches primarily address coarse-grained redundancy from exactly the same set formulas, neglecting that the data graph's inherent locality may lead to fine-grained duplicated edge checks. In fact, even unrelated set operations may check the same pair of vertices. This paper introduces the set union operation to the set-centric abstraction to fuse duplicated edge checks into one. It maintains the expressive power of relational algebra and previous optimizations while effectively avoids fine-grained redundancy in GPM tasks. Compared to state-of-the-art methods, our method achieves significant speedup on a V100 GPU cluster, demonstrating up to 305 × faster performance than the state-of-the-art GPM system G<sup>2</sup>Miner.</p>
			</div></div>
						
					<h2>SESSION: Synchronization and Concurrency Control II</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638497">Memory Bounds for Concurrent Bounded Queues</a></h3><ul class="DLauthors"><li class="nameList">Vitaly Aksenov</li><li class="nameList">Nikita Koval</li><li class="nameList">Petr Kuznetsov</li><li class="nameList Last">Anton Paramonov</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Concurrent data structures often require additional memory for handling synchronization issues in addition to memory for storing elements. Depending on the amount of this additional memory, implementations can be more or less <em>memory-friendly.</em> A <em>memory-optimal</em> implementation enjoys the minimal possible memory overhead, which, in practice, reduces cache misses and unnecessary memory reclamation.</p>
				<p>In this paper, we discuss the memory-optimality of non-blocking <em>bounded queues.</em> Essentially, we investigate the possibility of constructing an implementation that utilizes a pre-allocated array to store elements and constant memory overhead, e.g., two positioning counters for enqueue(..) and dequeue() operations. Such an implementation can be readily constructed when the ABA problem is precluded, e.g., assuming that the hardware supports LL/SC instructions or all inserted elements are distinct. However, in the general case, we show that a memory-optimal non-blocking bounded queue incurs linear overhead in the number of concurrent processes. These results not only provide helpful intuition for concurrent algorithm developers but also open a new research avenue on the memory-optimality phenomenon in concurrent data structures.</p>
				<p><em>The full version of this paper is available on arXiv [2].</em></p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638501">VERLIB: Concurrent Versioned Pointers</a></h3><ul class="DLauthors"><li class="nameList">Guy E. Blelloch</li><li class="nameList Last">Yuanhao Wei</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Recent work has shown how to augment any CAS-based concurrent data structure to support taking a snapshot of the current memory state. Taking the snapshot, as well as loads and CAS (Compare and Swap) operations, take constant time. Importantly, such snapshotting can be used to easily implement linearizable queries, such as range queries, over any part of a data structure.</p>
				<p>In this paper, we make two significant improvements over this approach. The first improvement removes a subtle and hard to reason about restriction that was needed to avoid a level of indirection on pointers. We introduce an approach, which we refer to as <em>indirection-on-need</em>, that removes the restriction, but yet almost always avoids indirection. The second improvement is to efficiently support snapshotting with lock-free locks. This requires supporting an idempotent CAS. We show a particularly simple solution to the problem that leverages the data structures used for snapshotting.</p>
				<p>Based on these ideas we implemented an easy-to-use C++ library, verlib, centered around a <em>versioned pointer</em> type. The library works with lock (standard or lock-free) and CAS based algorithms, or any combination. Converting existing concurrent data-structures to use the library takes minimal effort. We present results for experiments that use verlib to convert state-of-the-art data structures for ordered maps (a B-tree), radix-ordered maps (an ART-tree), and unordered maps (an optimized hash table) to be snapshottable. The snapshottable versions perform almost as well as the original versions and far outperform any previous implementations that support atomic range queries.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638504">Practical Hardware Transactional vEB Trees</a></h3><ul class="DLauthors"><li class="nameList">Mohammad Khalaji</li><li class="nameList">Trevor Brown</li><li class="nameList">Khuzaima Daudjee</li><li class="nameList Last">Vitaly Aksenov</li></ul><div class="DLabstract"><div style="display:inline">
				<p><em>van Emde Boas</em> (vEB) trees are sequential data structures optimized for extremely fast predecessor and successor queries. Such queries are an important incentive to use <em>ordered</em> sets or maps such as vEB trees. All operations in a vEB tree are doubly logarithmic in the universe size. Attempts to implement concurrent vEB trees have either simplified their structure in a way that eliminated their ability to perform fast predecessor and successor queries, or have otherwise compromised on doubly logarithmic complexity. In this work, we leverage <em>Hardware Transactional Memory</em> (HTM) to implement vEB tree-based sets and maps in which operations are doubly logarithmic in the absence of contention. Our proposed concurrent vEB tree is the first to implement <em>recursive summaries</em>, the key algorithmic component of fast predecessor and successor operations. Through extensive experiments, we demonstrate that our algorithm outperforms state-of-the-art concurrent maps by an average of 5× in a moderately skewed workload, and the single-threaded C++ standard ordered map and its unordered map by 70% and 14%, respectively. And, it does so while using two orders of magnitude less memory than traditional vEB trees.</p>
			</div></div>
						
					<h2>SESSION: ML Workloads</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638471">Tetris: Accelerating Sparse Convolution by Exploiting Memory Reuse on GPU</a></h3><ul class="DLauthors"><li class="nameList">Xiaoyan Liu</li><li class="nameList">Xuegui Zheng</li><li class="nameList">Hailong Yang</li><li class="nameList">Zhongzhi Luan</li><li class="nameList Last">Depei Qian</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Convolutional neural networks (CNNs) have achieved remarkable success in various application fields. Although model compression techniques mitigate the ever-increasing resource demands of large CNN models, the compressed models usually exhibit irregular memory access and unstructured sparsity, which are difficult for dominant operators such as sparse convolution to achieve expected performance speedup on popular inference platforms such as GPU. In this paper, we propose <em>Tetris</em>, an efficient sparse convolution approach optimized for GPU. <em>Tetris</em> first fully exploits the input reuse opportunity of sparse convolution to reduce the memory accesses to global memory. It then adopts a stride packed filter (SPF) format and a bank-sensing reorganization scheme to eliminate the irregular memory accesses caused by unstructured sparsity. It also leverages a filter group reorder technique to address load imbalance among threads, and a parameter tuning method to determine the optimal parameters of the sparse convolution implementation. The experiment results show that <em>Tetris</em> outperforms dense/sparse convolution libraries and cutting-edge implementations with promising performance speedup.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638502">Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips</a></h3><ul class="DLauthors"><li class="nameList">Ismet Dagli</li><li class="nameList Last">Mehmet E. Belviranli</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Two distinguishing features of state-of-the-art mobile and autonomous systems are: 1) There are often multiple workloads, mainly deep neural network (DNN) inference, running <em>concurrently</em> and <em>continuously.</em> 2) They operate on shared memory System-on-Chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art systems lack efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose <em>HaX-CoNN</em>, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within an SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find <em>optimal</em> schedules. We evaluate <em>HaX-CoNN</em> on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that <em>HaX-CoNN</em> can minimize memory contention by up to 45% and improve total latency and throughput by up to 32% and 29%, respectively, compared to the state-of-the-art.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638505">Training one DeePMD Model in Minutes: a Step towards Online Learning</a></h3><ul class="DLauthors"><li class="nameList">Siyu Hu</li><li class="nameList">Tong Zhao</li><li class="nameList">Qiuchen Sha</li><li class="nameList">Enji Li</li><li class="nameList">Xiangyu Meng</li><li class="nameList">Liping Liu</li><li class="nameList">Lin-Wang Wang</li><li class="nameList">Guangming Tan</li><li class="nameList Last">Weile Jia</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Neural Network Molecular Dynamics (NNMD) has become a major approach in material simulations, which can speedup the molecular dynamics (MD) simulation for thousands of times, while maintaining <em>ab initio</em> accuracy, thus has a potential to fundamentally change the paradigm of material simulations. However, there are two time-consuming bottlenecks of the NNMD developments. One is the data access of <em>ab initio</em> calculation results. The other, which is the focus of the current work, is reducing the training time of NNMD model. The training of NNMD model is different from most other neural network training because the atomic force (which is related to the gradient of the network) is an important physical property to be fit. Tests show the traditional stochastic gradient methods, like the Adam algorithms, cannot efficiently deploy the multisample minibatch algorithm. As a result, a typical training (taking the Deep Potential Molecular Dynamics (DeePMD) as an example) can take many hours. In this work, we designed a heuristic minibatch quasi-Newtonian optimizer based on Extended Kalman Filter method. An early reduction of gradient and error is adopted to reduce memory footprint and communication. The memory footprint, communication and settings of hyper-parameters of this new method are analyzed in detail. Computational innovations such as customized kernels of the symmetry-preserving descriptor are applied to exploit the computing power of the heterogeneous architecture. Experiments are performed on 8 different datasets representing different real case situations, and numerical results show that our new method has an average speedup of 32.2 compared to the Reorganized Layer-wised Extended Kalman Filter with 1 GPU, reducing the absolute training time of one DeePMD model from hours to several minutes, making it one step toward online training.</p>
			</div></div>
						
					<h2>SESSION: Parallel Algorithms</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638475">ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms</a></h3><ul class="DLauthors"><li class="nameList">Magdalen Dobson Manohar</li><li class="nameList">Zheqi Shen</li><li class="nameList">Guy Blelloch</li><li class="nameList">Laxman Dhulipala</li><li class="nameList">Yan Gu</li><li class="nameList">Harsha Vardhan Simhadri</li><li class="nameList Last">Yihan Sun</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Approximate nearest-neighbor search (ANNS) algorithms are a key part of the modern deep learning stack due to enabling efficient similarity search over high-dimensional vector space representations (i.e., embeddings) of data. Among various ANNS algorithms, graph-based algorithms are known to achieve the best throughput-recall tradeoffs. Despite the large scale of modern ANNS datasets, existing parallel graph-based implementations suffer from significant challenges to scale to large datasets due to heavy use of locks and other sequential bottlenecks, which 1) prevents them from efficiently scaling to a large number of processors, and 2) results in non-determinism that is undesirable in certain applications.</p>
				<p>In this paper, we introduce ParlayANN, a library of deterministic and parallel graph-based approximate nearest neighbor search algorithms, along with a set of useful tools for developing such algorithms. In this library, we develop novel parallel implementations for four state-of-the-art graph-based ANNS algorithms that scale to billion-scale datasets. Our algorithms are deterministic and achieve high scalability across a diverse set of challenging datasets. In addition to the new algorithmic ideas, we also conduct a detailed experimental study of our new algorithms as well as two existing non-graph approaches. Our experimental results both validate the effectiveness of our new techniques, and lead to a comprehensive comparison among ANNS algorithms on large scale datasets with a list of interesting findings.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638508">Parallel k-Core Decomposition with Batched Updates and Asynchronous Reads</a></h3><ul class="DLauthors"><li class="nameList">Quanquan C. Liu</li><li class="nameList">Julian Shun</li><li class="nameList Last">Igor Zablotchi</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Maintaining a dynamic <em>k</em>-core decomposition is an important problem that identifies dense subgraphs in dynamically changing graphs. Recent work by Liu et al. [SPAA 2022] presents a parallel batch-dynamic algorithm for maintaining an approximate <em>k</em>-core decomposition. In their solution, both reads and updates need to be batched, and therefore each type of operation can incur high latency waiting for the other type to finish. To tackle most real-world workloads, which are dominated by reads, this paper presents a novel hybrid concurrent-parallel dynamic <em>k</em>-core data structure where asynchronous reads can proceed concurrently with batches of updates, leading to significantly lower read latencies. Our approach is based on tracking causal dependencies between updates, so that causally related groups of updates appear atomic to concurrent readers. Our data structure guarantees linearizability and liveness for both reads and updates, and maintains the same approximation guarantees as prior work. Our experimental evaluation on a 30-core machine shows that our approach reduces read latency by orders of magnitude compared to the batch-dynamic algorithm, up to a (4.05 · 10<sup>5</sup>)-factor. Compared to an unsynchronized (non-linearizable) baseline, our read latency overhead is only up to a 3.21-factor greater, while improving accuracy of coreness estimates by up to a factor of 52.7.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638483">Parallel Integer Sort: Theory and Practice</a></h3><ul class="DLauthors"><li class="nameList">Xiaojun Dong</li><li class="nameList">Laxman Dhulipala</li><li class="nameList">Yan Gu</li><li class="nameList Last">Yihan Sun</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Integer sorting is a fundamental problem in computer science. This paper studies parallel integer sort both in theory and in practice. In theory, we show tighter bounds for a class of existing practical integer sort algorithms, which provides a solid theoretical foundation for their widespread usage in practice and strong performance. In practice, we design a new integer sorting algorithm, DovetailSort, that is theoretically-efficient and has good practical performance.</p>
				<p>In particular, DovetailSort overcomes a common challenge in existing parallel integer sorting algorithms, which is the difficulty of detecting and taking advantage of duplicate keys. The key insight in DovetailSort is to combine algorithmic ideas from both integer- and comparison-sorting algorithms. In our experiments, DovetailSort achieves competitive or better performance than existing state-of-the-art parallel integer and comparison sorting algorithms on various synthetic and real-world datasets.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638506">Fast American Option Pricing using Nonlinear Stencils</a></h3><ul class="DLauthors"><li class="nameList">Zafar Ahmad</li><li class="nameList">Reilly Browne</li><li class="nameList">Rezaul Chowdhury</li><li class="nameList">Rathish Das</li><li class="nameList">Yushen Huang</li><li class="nameList Last">Yimin Zhu</li></ul><div class="DLabstract"><div style="display:inline">
				<p>We study the binomial, trinomial, and Black-Scholes-Merton models of option pricing. We present fast parallel discrete-time finite-difference algorithms for American call option pricing under the binomial and trinomial models and American put option pricing under the Black-Scholes-Merton model. For <em>T</em>-step finite differences, each algorithm runs in <em>O</em> <em>(T</em> log<sup>2</sup> <em>T)</em>/<em>p</em> + <em>T)</em> time under a greedy scheduler on <em>p</em> processing cores, which is a significant improvement over the Θ <em>(T</em><sup>2</sup>/<em>p)</em> + Ω <em>(T</em> log <em>T)</em> time taken by the corresponding state-of-the-art parallel algorithm. Even when run on a single core, the <em>O</em> <em>(T</em> log<sup>2</sup> <em>T)</em> time taken by our algorithms is asymptotically much smaller than the Θ <em>(T</em><sup>2</sup>) running time of the fastest known serial algorithms. Implementations of our algorithms significantly outperform the fastest implementations of existing algorithms in practice, e.g., when run for <em>T</em> ≈ 1000 steps on a 48-core machine, our algorithm for the binomial model runs at least 15× faster than the fastest existing parallel program for the same model with the speedup factor gradually reaching beyond 500× for <em>T</em> ≈ 0.5 × 10<sup>6</sup>. It saves more than 80% energy when <em>T</em> ≈ 4000, and more than 99% energy for <em>T</em> &gt; 60,000.</p>
				<p>Our algorithms can be viewed as solving a class of nonlinear 1D stencil (i.e., finite-difference) computation problems efficiently using the Fast Fourier Transform (FFT). To our knowledge, ours are the first algorithms to handle such stencils in <em>o</em> <em>(T</em><sup>2</sup>) time. These contributions are of independent interest as stencil computations have a wide range of applications beyond quantitative finance.</p>
			</div></div>
						
					<h2>SESSION: Optimizing for Memory</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638476">ConvStencil: Transform Stencil Computation to Matrix Multiplication on Tensor Cores</a></h3><ul class="DLauthors"><li class="nameList">Yuetao Chen</li><li class="nameList">Kun Li</li><li class="nameList">Yuhao Wang</li><li class="nameList">Donglin Bai</li><li class="nameList">Lei Wang</li><li class="nameList">Lingxiao Ma</li><li class="nameList">Liang Yuan</li><li class="nameList">Yunquan Zhang</li><li class="nameList">Ting Cao</li><li class="nameList Last">Mao Yang</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Tensor Core Unit (TCU) is increasingly integrated into modern high-performance processors to enhance matrix multiplication performance. However, constrained to its over-specification, its potential for improving other critical scientific operations like stencil computations remains untapped.</p>
				<p>This paper presents ConvStencil<sup>1</sup>, a novel stencil computing system designed to efficiently transform stencil computation to matrix multiplication on Tensor Cores. We first develop a performance model for ConvStencil to guide algorithm design and optimization on TCUs. Based on this model, we propose three techniques: (1) Memory-efficient Layout Transformation using the stencil2row method; (2) Computation-dense Compute Adaptation with Dual Tessellation and kernel fusion; and (3) Performance-boosting Conflict Removal using a Lookup Table and Dirty Bits Padding. ConvStencil outperforms other stencil optimization frameworks, achieving significant speedups compared to solutions like AMOS, cuDNN, Brick, DRStencil, and TCStencil. By transforming stencil computation on Tensor Cores, ConvStencil promises to improve the performance of various scientific and engineering applications.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638492">CPMA: An Efficient Batch-Parallel Compressed Set Without Pointers</a></h3><ul class="DLauthors"><li class="nameList">Brian Wheatman</li><li class="nameList">Randal Burns</li><li class="nameList">Aydin Buluc</li><li class="nameList Last">Helen Xu</li></ul><div class="DLabstract"><div style="display:inline">
				<p>This paper introduces the <em>batch-parallel Compressed Packed Memory Array</em> (CPMA), a compressed, dynamic, ordered set data structure based on the Packed Memory Array (PMA). Traditionally, batch-parallel sets are built on pointer-based data structures such as trees because pointer-based structures enable fast parallel unions via pointer manipulation. When compared with cache-optimized trees, PMAs were slower to update but faster to scan.</p>
				<p>The batch-parallel CPMA overcomes this tradeoff between updates and scans by optimizing for cache-friendliness. On average, the CPMA achieves 3× faster batch-insert throughput and 4× faster range-query throughput compared with compressed PaC-trees, a state-of-the-art batch-parallel set library based on cache-optimized trees.</p>
				<p>We further evaluate the CPMA compared with compressed PaC-trees and Aspen, a state-of-the-art system, on a real-world application of dynamic-graph processing. The CPMA is on average 1.2× faster on a suite of graph algorithms and 2× faster on batch inserts when compared with compressed PaC-trees. Furthermore, the CPMA is on average 1.3× faster on graph algorithms and 2× faster on batch inserts compared with Aspen.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638499">Gallatin: A General-Purpose GPU Memory Manager</a></h3><ul class="DLauthors"><li class="nameList">Hunter Mccoy</li><li class="nameList Last">Prashant Pandey</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Dynamic memory management is critical for efficiently porting modern data processing pipelines to GPUs. However, building a general-purpose dynamic memory manager on GPUs is challenging due to the massive parallelism and weak memory coherence. Existing state-of-the-art GPU memory managers, Ouroboros and Reg-Eff, employ traditional data structures such as arrays and linked lists to manage memory objects. They build specialized pipelines to achieve performance for a fixed set of allocation sizes and fall back to the CUDA allocator for allocating large sizes. In the process, they lose general-purpose usability and fail to support critical applications such as streaming graph processing.</p>
				<p>In this paper, we introduce Gallatin, a general-purpose and high-performance GPU memory manager. Gallatin uses the van Emde Boas (vEB) tree data structure to manage memory objects efficiently and supports allocations of any size. Furthermore, we develop a highly-concurrent GPU implementation of the vEB tree which can be broadly used in other GPU applications. It supports constant time insertions, deletions, and successor operations for a given memory size.</p>
				<p>In our evaluation, we compare Gallatin with state-of-the-art specialized allocator variants. Gallatin is up to 374× faster on single-sized allocations and up to 264× faster on mixed-size allocations than the next-best allocator. In scalability benchmarks, Gallatin is up to 254× times faster than the next-best allocator as the number of threads increases. For the graph benchmarks, Gallatin is 1.5× faster than the state-of-the-art for bulk insertions, slightly faster for bulk deletions, and is 3× faster than the next-best allocator for all graph expansion tests.</p>
			</div></div>
						
					<h2>SESSION: Linear Algebra</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638470">A Row Decomposition-based Approach for Sparse Matrix Multiplication on GPUs</a></h3><ul class="DLauthors"><li class="nameList">Meng Pang</li><li class="nameList">Xiang Fei</li><li class="nameList">Peng Qu</li><li class="nameList">Youhui Zhang</li><li class="nameList Last">Zhaolin Li</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Sparse-Matrix Dense-Matrix Multiplication (SpMM) and Sampled Dense Dense Matrix Multiplication (SDDMM) are important sparse kernels in various computation domains. The uneven distribution of nonzeros in the sparse matrix and the tight data dependence between sparse and dense matrices make it a challenge to run sparse matrix multiplication efficiently on GPUs. By analyzing the aforementioned problems, we propose a row decomposition (RoDe)-based approach to optimize the two kernels on GPUs, using the standard Compressed Sparse Row (CSR) format. Specifically, RoDe divides the sparse matrix rows into regular parts and residual parts, to fully optimize their computations separately. We also devise the corresponding load balancing and finegrained pipelining technologies. Profiling results show that RoDe can achieve more efficient memory access and reduce warp stall cycles significantly. Compared to the state-of-the-art (SOTA) alternatives, RoDe achieves a speedup of up to 7.86× with a geometric mean of 1.45× for SpMM, and a speedup of up to 8.99× with a geometric mean of 1.49× for SDDMM; the dataset is SuiteSparse. RoDe also outperforms its counterpart in the deep learning dataset. Furthermore, its preprocessing overhead is significantly smaller, averaging only 16% of the SOTA.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638489">Fast Kronecker Matrix-Matrix Multiplication on GPUs</a></h3><ul class="DLauthors"><li class="nameList">Abhinav Jangda</li><li class="nameList Last">Mohit Yadav</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Kronecker Matrix-Matrix Multiplication (Kron-Matmul) is the multiplication of a matrix with the Kronecker Product of several smaller matrices. Kron-Matmul is a core operation for many scientific and machine learning computations. State-of-the-art Kron-Matmul implementations utilize existing tensor algebra operations, such as matrix multiplication, transpose, and tensor matrix multiplication. However, this design choice prevents several Kron-Matmul specific optimizations, thus, leaving significant performance on the table.</p>
				<p>To address this issue, we present FastKron, an efficient technique for Kron-Matmul on single and multiple GPUs. FastKron is independent of linear algebra operations enabling several new optimizations for Kron-Matmul. Thus, it performs up to 40.7× and 7.85× faster than existing implementations on 1 and 16 GPUs respectively.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638496">Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication</a></h3><ul class="DLauthors"><li class="nameList">Lukas Gianinazzi</li><li class="nameList">Alexandros Nikolaos Ziogas</li><li class="nameList">Langwen Huang</li><li class="nameList">Piotr Luczynski</li><li class="nameList">Saleh Ashkboosh</li><li class="nameList">Florian Scheidl</li><li class="nameList">Armon Carigiet</li><li class="nameList">Chio Ge</li><li class="nameList">Nabil Abubaker</li><li class="nameList">Maciej Besta</li><li class="nameList">Tal Ben-Nun</li><li class="nameList Last">Torsten Hoefler</li></ul><div class="DLabstract"><div style="display:inline">
				<p>We propose a novel approach to iterated sparse matrix dense matrix multiplication, a fundamental computational kernel in scientific computing and graph neural network training. In cases where matrix sizes exceed the memory of a single compute node, data transfer becomes a bottleneck. An approach based on dense matrix multiplication algorithms leads to sub-optimal scalability and fails to exploit the sparsity in the problem. To address these challenges, we propose decomposing the sparse matrix into a small number of highly structured matrices called <em>arrow</em> matrices, which are connected by permutations. Our approach enables communication-avoiding multiplications, achieving a polynomial reduction in communication volume per iteration for matrices corresponding to planar graphs and other minor-excluded families of graphs. Our evaluation demonstrates that our approach outperforms a state-of-the-art method for sparse matrix multiplication on matrices with hundreds of millions of rows, offering near-linear strong and weak scaling.</p>
			</div></div>
						
					<h2>SESSION: Applications</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638465">FastFold: Optimizing AlphaFold Training and Inference on GPU Clusters</a></h3><ul class="DLauthors"><li class="nameList">Shenggan Cheng</li><li class="nameList">Xuanlei Zhao</li><li class="nameList">Guangyang Lu</li><li class="nameList">Jiarui Fang</li><li class="nameList">Tian Zheng</li><li class="nameList">Ruidong Wu</li><li class="nameList">Xiwen Zhang</li><li class="nameList">Jian Peng</li><li class="nameList Last">Yang You</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Protein structure prediction helps to understand gene translation and protein function, which is of growing interest and importance in structural biology. The AlphaFold model, which used transformer architecture to achieve atomic-level accuracy in protein structure prediction, was a significant breakthrough. However, training and inference of AlphaFold model are challenging due to its high computation and memory cost. In this work, we present FastFold, an efficient implementation of AlphaFold for both training and inference. We propose <em>Dynamic Axial Parallelism</em> (DAP) as a novel model parallelism method. Additionally, we have implemented a series of low-level optimizations aimed at reducing communication, computation, and memory costs. These optimizations include <em>Duality Async Operations</em>, highly optimized kernels, and <em>AutoChunk</em> (an automated search algorithm finds the best chunk strategy to reduce memory peaks). Experimental results show that FastFold can efficiently scale to more GPUs using <em>DAP</em> and reduces overall training time from 11 days to 67 hours and achieves 7.5 ~ 9.5× speedup for long-sequence inference. Furthermore, AutoChunk can reduce memory cost by over 80% during inference by automatically partitioning the intermediate tensors during the computation.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638474">AGAThA: Fast and Efficient GPU Acceleration of Guided Sequence Alignment for Long Read Mapping</a></h3><ul class="DLauthors"><li class="nameList">Seongyeon Park</li><li class="nameList">Junguk Hong</li><li class="nameList">Jaeyong Song</li><li class="nameList">Hajin Kim</li><li class="nameList">Youngsok Kim</li><li class="nameList Last">Jinho Lee</li></ul><div class="DLabstract"><div style="display:inline">
				<p>With the advance in genome sequencing technology, the lengths of deoxyribonucleic acid (DNA) sequencing results are rapidly increasing at lower prices than ever. However, the longer lengths come at the cost of a heavy computational burden on aligning them. For example, aligning sequences to a human reference genome can take tens or even hundreds of hours. The current de facto standard approach for alignment is based on the guided dynamic programming method. Although this takes a long time and could potentially benefit from high-throughput graphic processing units (GPUs), the existing GPU-accelerated approaches often compromise the algorithm's structure, due to the GPU-unfriendly nature of the computational pattern. Unfortunately, such compromise in the algorithm is not tolerable in the field, because sequence alignment is a part of complicated bioinformatics analysis pipelines. In such circumstances, we propose AGAThA, an exact and efficient GPU-based acceleration of guided sequence alignment. We diagnose and address the problems of the algorithm being unfriendly to GPUs, which comprises strided/redundant memory accesses and workload imbalances that are difficult to predict. According to the experiments on modern GPUs, AGAThA achieves 18.8× speedup against the CPU-based baseline, 9.6× against the best GPU-based baseline, and 3.6× against GPU-based algorithms with different heuristics.</p>
			</div></div>
						
					<h2>POSTER SESSION: Posters</h2>
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638495">POSTER: Accelerating High-Precision Integer Multiplication used in Cryptosystems with GPUs</a></h3><ul class="DLauthors"><li class="nameList">Zhuoran Ji</li><li class="nameList">Zhaorui Zhang</li><li class="nameList">Jiming Xu</li><li class="nameList Last">Lei Ju</li></ul><div class="DLabstract"><div style="display:inline">
				<p>High-precision integer multiplication is crucial in privacy-preserving computational techniques but poses acceleration challenges on GPUs due to its complexity and the diverse bit lengths in cryptosystems. This paper introduces <em>GIM</em>, an efficient high-precision integer multiplication algorithm accelerated with GPUs. It employs a novel segmented integer multiplication algorithm that separates implementation details from bit length, facilitating code optimizations. We also present a computation diagram to analyze parallelization strategies, leading to a series of enhancements. Experiments demonstrate that this approach achieves a 4.47× speedup over the commonly used baseline.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638486">POSTER: Enabling Extreme-Scale Phase Field Simulation with In-situ Feature Extraction</a></h3><ul class="DLauthors"><li class="nameList">Zhichen Feng</li><li class="nameList">Jialin Li</li><li class="nameList">Yaqian Gao</li><li class="nameList">Shaobo Tian</li><li class="nameList">Huang Ye</li><li class="nameList Last">Jian Zhang</li></ul><div class="DLabstract"><div style="display:inline">
				<p>In this paper, we present an integrated framework composed of a highly efficient phase field simulator and an in-situ feature extraction library. This novel framework enables us to conduct extreme-scale micro-structure evolution simulations while the characteristic features of each individual grain are extracted on the fly. After systematic design and optimization on the new generation Sunway supercomputer, the code scales up to 39 million cores and achieves 582 PFlops in double precision and 637 POps in mixed precision.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638485">POSTER: FineCo: Fine-grained Heterogeneous Resource Management for Concurrent DNN Inferences</a></h3><ul class="DLauthors"><li class="nameList">Lixian Ma</li><li class="nameList">Haoruo Chen</li><li class="nameList">En Shao</li><li class="nameList">Leping Wang</li><li class="nameList">Quan Chen</li><li class="nameList Last">Guangming Tan</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Co-locating multiple DNN servings to share GPU resource is widely used to improve resource utilization while guaranteeing user QoS. Existing GPU sharing mechanism is restricted to model level, and fluctuations in kernel-level resource demands highlight a suboptimal utilization of the current sharing mechanism. We design a multi-DNN serving system, <em>FineCo</em>, that leverages a novel fine-grained resource sharing mechanism to optimize concurrent inference without modifications to the hardware or operating system. Our prototype implementation demonstrates that <em>FineCo</em> achieves up to 40% throughput improvement over the state-of-the-art work.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638467">POSTER: Optimizing Collective Communications with Error-bounded Lossy Compression for GPU Clusters</a></h3><ul class="DLauthors"><li class="nameList">Jiajun Huang</li><li class="nameList">Sheng Di</li><li class="nameList">Xiaodong Yu</li><li class="nameList">Yujia Zhai</li><li class="nameList">Jinyang Liu</li><li class="nameList">Yafan Huang</li><li class="nameList">Ken Raffenetti</li><li class="nameList">Hui Zhou</li><li class="nameList">Kai Zhao</li><li class="nameList">Zizhong Chen</li><li class="nameList">Franck Cappello</li><li class="nameList">Yanfei Guo</li><li class="nameList Last">Rajeev Thakur</li></ul><div class="DLabstract"><div style="display:inline">
				<p>GPU-aware collective communication has become a major bottleneck for modern computing platforms as GPU computing power rapidly rises. To address this issue, traditional approaches integrate lossy compression directly into GPU-aware collectives, which still suffer from serious issues such as underutilized GPU devices and uncontrolled data distortion. In this paper, we propose <em>GPU-LCC</em>, a general framework that designs and optimizes GPU-aware, compression-enabled collectives with well-controlled error propagation. To validate our framework, we evaluate the performance on up to 64 NVIDIA A100 GPUs with real-world applications and datasets. Experimental results demonstrate that our <em>GPU-LCC</em>-accelerated collective computation (Allreduce), can outperform NCCL as well as Cray MPI by up to 3.4× and 18.7×, respectively. Furthermore, our accuracy evaluation with an image-stacking application confirms the high reconstructed data quality of our accuracy-aware framework.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638500">POSTER: Optimizing Sparse Tensor Contraction with Revisiting Hash Table Design</a></h3><ul class="DLauthors"><li class="nameList">Guofeng Feng</li><li class="nameList">Weile Jia</li><li class="nameList">Ninghui Sun</li><li class="nameList">Guangming Tan</li><li class="nameList Last">Jiajia Li</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Sparse tensor contraction (SpTC) serves as an essential operation in high-performance applications. The high dimensionality of sparse tensors makes SpTC fundamentally challenging in aspects such as costly multidimensional index search, extensive intermediate output data, and indirect addressing. Previous state-of-the-art work addresses some of these challenges through hash-table implementation. In this paper, we propose a hash-table based and fully optimized SpTC by providing a more carefully designed customized hash table design, proposing an architecture-aware algorithm for hash table selection with size prediction, applying cross-stage optimizations to exploit shared information and avoid redundant operations. Evaluating on a set of tensors extracted from the real world, our method can achieve superior speedup and reduce the memory footprint substantially compared to the current state-of-the-art work.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638480">POSTER: LLM-PQ:Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization</a></h3><ul class="DLauthors"><li class="nameList">Juntao Zhao</li><li class="nameList">Borui Wan</li><li class="nameList">Chuan Wu</li><li class="nameList">Yanghua Peng</li><li class="nameList Last">Haibin Lin</li></ul><div class="DLabstract"><div style="display:inline">
				<p>The immense sizes of Large-scale language models (LLMs) have led to high resource demand and cost for running the models. Though the models are largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous cluster with a mix of available high- and low-capacity GPUs can potentially substantially reduce the serving cost. This paper proposes LLM-PQ, a system that advocates adaptive model quantization and phase-aware partition to improve LLM serving efficiency on heterogeneous GPU clusters. Extensive experiments on production inference workloads demonstrate throughput improvement in inference, showing great advantages over state-of-the-art works.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638494">POSTER: OCToPus: Semantic-aware Concurrency Control for Blockchain Transactions</a></h3><ul class="DLauthors"><li class="nameList">dePaul Miller</li><li class="nameList">Henry F. Korth</li><li class="nameList Last">Roberto Palmieri</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Many blockchain implementations offer APIs to send and receive money between accounts exclusively. In this paper, we introduce OCToPus, a deterministic concurrency control scheme that uses a semantic-aware fast path and a GPU-accelerated directed acyclic graph-based fallback path to parallelize the execution of a block aggressively.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638481">POSTER: Pattern-Aware Sparse Communication for Scalable Recommendation Model Training</a></h3><ul class="DLauthors"><li class="nameList">Jiaao He</li><li class="nameList">Shengqi Chen</li><li class="nameList Last">Jidong Zhai</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Recommendation models are an important category of deep learning models whose size is growing enormous. They consist of a sparse part with TBs of memory footprint and a dense part that demands PFLOPs of computing capability to train. Unfortunately, the high sparse communication cost to re-organize data for different parallel strategies of the two parts impedes the scalability in training.</p>
				<p>Based on observations of sparse access patterns, we design a two-fold fine-grained parallel strategy to accelerate sparse communication. A performance model is built to select an optimal set of items that are replicated across all GPUs so that all-to-all communication volume is reduced, while keeping memory consumption acceptable. The all-to-all overhead is further reduced by parallel scheduling techniques. In our evaluation on 32 GPUs over real-world datasets, 2.16 -- 16.8× end-to-end speedup is achieved over the baselines.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638488">POSTER: ParGNN: Efficient Training for Large-Scale Graph Neural Network on GPU Clusters</a></h3><ul class="DLauthors"><li class="nameList">Shunde Li</li><li class="nameList">Junyu Gu</li><li class="nameList">Jue Wang</li><li class="nameList">Tiechui Yao</li><li class="nameList">Zhiqiang Liang</li><li class="nameList">Yumeng Shi</li><li class="nameList">Shigang Li</li><li class="nameList">Weiting Xi</li><li class="nameList">Shushen Li</li><li class="nameList">Chunbao Zhou</li><li class="nameList">Yangang Wang</li><li class="nameList Last">Xuebin Chi</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Full-batch graph neural network (GNN) training is essential for interdisciplinary applications. Large-scale graph data is usually divided into subgraphs and distributed across multiple compute units to train GNN. The state-of-the-art load balancing method based on direct graph partition is too rough to effectively achieve true load balancing on GPU clusters. We propose ParGNN, which employs a profiler-guided load balance workflow in conjunction with graph repartition to alleviate load imbalance and minimize communication traffic. Experiments have verified that ParGNN has the capability to scale to larger clusters.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638478">POSTER: RadiK: Scalable Radix Top-K Selection on GPUs</a></h3><ul class="DLauthors"><li class="nameList">Yifei Li</li><li class="nameList">Bole Zhou</li><li class="nameList">Jiejing Zhang</li><li class="nameList">Xuechao Wei</li><li class="nameList">Yinghan Li</li><li class="nameList Last">Yingda Chen</li></ul><div class="DLabstract"><div style="display:inline">
				<p>By identifying the <em>k</em> largest or smallest elements in a set of data, top-k selection is critical for modern high-performance databases and machine learning systems, especially with large data volumes. However, previous studies on its GPU implementation are mostly merge-based and rely heavily on the high-speed but size-limited on-chip memory, thereby resulting in a restricted upper bound on <em>k.</em> This paper introduces RadiK, a highly optimized GPU-parallel radix top-k selection that is scalable with <em>k</em>, input length, and batch size. With a carefully designed optimization framework targeting high memory bandwidth and resource utilization, RadiK supports far larger <em>k</em> than the prior art, achieving up to 2.5× speedup for non-batch queries and up to 4.8× speedup for batch queries. We also propose a lightweight refinement that strengthens the robustness of RadiK against skewed distributions by adaptively scaling the input elements.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638469">POSTER: RELAX: Durable Data Structures with Swift Recovery</a></h3><ul class="DLauthors"><li class="nameList">Almog Zur</li><li class="nameList">Nachshon Cohen</li><li class="nameList">Michal Friedman</li><li class="nameList Last">Erez Petrank</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Recent non-volatile main memory technology gave rise to an abundance of research on building persistent data structures, whose content can be recovered after a system crash. While there has been significant progress in making durable data structures efficient, shortening the length of the recovery phase after a crash has not received much attention. In this paper we present the RELAX general transformation. RELAX generates lock-free durable data structures that provide the best of both worlds: almost zero recovery time and high performance.</p>
			</div></div>
						
					
						<h3><a class="DLtitleLink" title="Full Citation in the ACM Digital Library" referrerpolicy="no-referrer-when-downgrade" href="https://dl.acm.org/doi/10.1145/3627535.3638482">POSTER: StructMG: A Fast and Scalable Structured Multigrid</a></h3><ul class="DLauthors"><li class="nameList">Yi Zong</li><li class="nameList">Xinliang Wang</li><li class="nameList">Haopeng Huang</li><li class="nameList">Chensong Zhang</li><li class="nameList">Xiaowen Xu</li><li class="nameList">Jian Sun</li><li class="nameList">Bowen Yan</li><li class="nameList">Qin Wang</li><li class="nameList">Sicong Li</li><li class="nameList">Zhaohui Ding</li><li class="nameList Last">Wei Xue</li></ul><div class="DLabstract"><div style="display:inline">
				<p>Parallel multigrid is widely used as preconditioners in solving large-scale sparse linear systems. However, the current multigrid library still needs more satisfactory performance for structured grid problems regarding speed and scalability. To this end, we design and implement StructMG, a fast and scalable multigrid that constructs hierarchical grids automatically based on the original matrix. As a preconditioner, StructMG can achieve both low cost per iteration and good convergence. Two idealized and five real-world problems from four application fields, including radiation hydrodynamics, petroleum reservoir simulation, numerical weather prediction, and solid mechanics, are evaluated on ARM and X86 platforms. In comparison to <em>hypre</em>'s multigrid preconditioners, StructMG achieves the fastest time-to-solutions in all cases with average speedups of 17.6x, 5.7x, 4.6x, 8.5x over SMG, PFMG, SysPFMG, and BoomerAMG, respectively. Additionally, StructMG significantly improves strong and weak scaling efficiencies in most tests.</p>
			</div></div>
						
					</div></div></body></html>